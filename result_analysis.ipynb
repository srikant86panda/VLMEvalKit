{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1167649/474653204.py\u001b[0m(51)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     49 \u001b[0;31m            \u001b[0mall_excel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 51 \u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_excel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m    \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig': ['InternVL2-1B_BLINK_sample_openai_result.xlsx'], 'grid_2x1': ['InternVL2-1B_BLINK_sample_grid_2x1_row1_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_2x1_row2_col1_openai_result.xlsx'], 'grid_1x2': ['InternVL2-1B_BLINK_sample_grid_1x2_row1_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_1x2_row1_col2_openai_result.xlsx'], 'grid_2x2': ['InternVL2-1B_BLINK_sample_grid_2x2_row1_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_2x2_row1_col2_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_2x2_row2_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_2x2_row2_col2_openai_result.xlsx'], 'grid_3x3': ['InternVL2-1B_BLINK_sample_grid_3x3_row1_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row1_col2_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row1_col3_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col2_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col3_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col1_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col2_openai_result.xlsx', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col3_openai_result.xlsx']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def get_group(filename):\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "def get_patch(filename):\n",
    "    pattern = r'row\\d+_col\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "\n",
    "base_dir = './outputs'\n",
    "model_name = 'InternVL2-1B' # llava_hf_v1.5_7b, InternVL2-1B, Idefics3-8B-Llama3, Phi-3.5-Vision\n",
    "dataset_name = 'BLINK_sample'\n",
    "file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*openai_result.xlsx')\n",
    "groups = {}\n",
    "for i in file_paths:\n",
    "    group = get_group(os.path.basename(i))\n",
    "    if group not in groups:\n",
    "        groups[group] = [os.path.basename(i)]\n",
    "    else:\n",
    "        groups[group].append(os.path.basename(i))\n",
    "\n",
    "all_combinations = []\n",
    "for r in range(1, len(groups.keys()) + 1):\n",
    "    all_combinations.extend(itertools.combinations(groups.keys(), r))\n",
    "\n",
    "final_dict = {}\n",
    "for gp_element in all_combinations:\n",
    "    all_excel = {}\n",
    "    for indi_element in gp_element:\n",
    "        for i in groups[indi_element]:\n",
    "            df = pd.read_excel(f'{base_dir}/{model_name}/{i}')\n",
    "            hit = df.hit.tolist()\n",
    "            all_excel[os.path.basename(i).replace('.xlsx', '')] = hit\n",
    "    import pdb; pdb.set_trace() \n",
    "    result = [int(any(values)) for values in zip(*all_excel.values())]\n",
    "    import pdb; pdb.set_trace()      \n",
    "    result = sum(result)/len(result)\n",
    "    final_dict['-'.join(gp_element)] = result\n",
    "# df = pd.DataFrame(list(final_dict.items()), columns=['Configuration', 'Value'])\n",
    "# df.to_csv(f'{base_dir}/{model_name}_sumary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('./outputs/Phi-3-Vision/Phi-3-Vision_RealWorldQA_grid_2x1_row2_col1_openai_result.xlsx')\n",
    "hit = df.hit.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48627450980392156"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sum(hit)/len(hit)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['InternVL2-1B_BLINK_sample_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x1_row1_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x1_row2_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_1x2_row1_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_1x2_row1_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x2_row1_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x2_row1_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x2_row2_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_2x2_row2_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row1_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row1_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row1_col3_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row2_col3_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col1_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col2_openai_result', 'InternVL2-1B_BLINK_sample_grid_3x3_row3_col3_openai_result'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_excel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def get_group(filename):\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "def get_patch(filename):\n",
    "    pattern = r'row\\d+_col\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "base_dir = '/home/srikapan/workspace/research/multiblock/VLMEvalKit/outputs'\n",
    "model_name = 'Phi-3.5-Vision' # llava_hf_v1.5_7b, InternVL2-1B, Idefics3-8B-Llama3, Phi-3.5-Vision\n",
    "dataset_name = 'RealWorldQA'\n",
    "for model_name in ['llava_hf_v1.5_7b', 'InternVL2-1B', 'Idefics3-8B-Llama3', 'Phi-3.5-Vision']:\n",
    "    file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*openai_result.xlsx')\n",
    "    groups = {}\n",
    "    for i in file_paths:\n",
    "        group = get_group(os.path.basename(i))\n",
    "        if group not in groups:\n",
    "            groups[group] = [os.path.basename(i)]\n",
    "        else:\n",
    "            groups[group].append(os.path.basename(i))\n",
    "\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(groups.keys()) + 1):\n",
    "        all_combinations.extend(itertools.combinations(groups.keys(), r))\n",
    "\n",
    "    final_dict = {}\n",
    "    for gp_element in all_combinations:\n",
    "        all_excel = {}\n",
    "        for indi_element in gp_element:\n",
    "            for i in groups[indi_element]:\n",
    "                df = pd.read_excel(f'{base_dir}/{model_name}/{i}')\n",
    "                hit = df.hit.tolist()\n",
    "                all_excel[os.path.basename(i).replace('.xlsx', '')] = hit\n",
    "        result = [int(any(values)) for values in zip(*all_excel.values())]\n",
    "        result = sum(result)/len(result)\n",
    "        final_dict['-'.join(gp_element)] = result\n",
    "    df = pd.DataFrame(list(final_dict.items()), columns=['Configuration', 'Value'])\n",
    "    df.to_csv(f'{base_dir}/{model_name}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "\n",
    "bleu_scorer = Bleu(n=4)\n",
    "\n",
    "def get_score(reference_captions, generated_captions):\n",
    "    reference_captions = {\"0\": ast.literal_eval(reference_captions)}\n",
    "    generated_captions = {\"0\": [generated_captions]}\n",
    "\n",
    "    scores, detailed_scores = bleu_scorer.compute_score(reference_captions, generated_captions)\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def get_group(filename):\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "def get_patch(filename):\n",
    "    pattern = r'row\\d+_col\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        extracted = match.group()\n",
    "    else:\n",
    "        extracted = 'orig'\n",
    "    return extracted\n",
    "\n",
    "\n",
    "base_dir = './outputs'\n",
    "dataset_name = 'COCO_VAL_sample'\n",
    "for model_name in ['llava-onevision-qwen2-0.5b-ov-hf', 'InternVL2-1B', 'InternVL2_5-1B', 'Phi-3-Vision']:\n",
    "    file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*.xlsx')\n",
    "    groups = {}\n",
    "    for i in file_paths:\n",
    "        group = get_group(os.path.basename(i))\n",
    "        if group not in groups:\n",
    "            groups[group] = [os.path.basename(i)]\n",
    "        else:\n",
    "            groups[group].append(os.path.basename(i))\n",
    "\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(groups.keys()) + 1):\n",
    "        all_combinations.extend(itertools.combinations(groups.keys(), r))\n",
    "\n",
    "    final_dict = {}\n",
    "    for gp_element in all_combinations:\n",
    "        all_excel = {}\n",
    "        for indi_element in gp_element:\n",
    "            for i in groups[indi_element]:\n",
    "                df = pd.read_excel(f'{base_dir}/{model_name}/{i}')\n",
    "                df.loc[:, 'scores'] = df[['answer', 'prediction']].apply(lambda x: get_score(x['answer'], x['prediction']), axis=1)\n",
    "                scores = df.scores.tolist()\n",
    "                all_excel[os.path.basename(i).replace('.xlsx', '')] = scores\n",
    "        result = [(max(values)) for values in zip(*all_excel.values())]\n",
    "        result = sum(result)\n",
    "        final_dict['-'.join(gp_element)] = result\n",
    "    df = pd.DataFrame(list(final_dict.items()), columns=['Configuration', 'Value'])\n",
    "    df.to_csv(f'{base_dir}/{model_name}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "495    0\n",
      "496    0\n",
      "497    0\n",
      "498    0\n",
      "499    0\n",
      "Name: hit, Length: 500, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indi_element \u001b[38;5;129;01min\u001b[39;00m gp_element:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m groups[indi_element]:\n\u001b[0;32m---> 40\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhit)\n\u001b[1;32m     42\u001b[0m         hit \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mhit\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1567\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:553\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mReader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    552\u001b[0m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:563\u001b[0m, in \u001b[0;36mBaseExcelReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[1;32m    560\u001b[0m     handle\u001b[38;5;241m=\u001b[39mfilepath_or_buffer, compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, (ExcelFile, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workbook_class)):\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workbook_class):\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def get_group(filename):\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    return match.group() if match else 'orig'\n",
    "\n",
    "def get_patch(filename):\n",
    "    pattern = r'row\\d+_col\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    return match.group() if match else 'orig'\n",
    "\n",
    "base_dir = './outputs'\n",
    "dataset_name = 'BLINK_sample'\n",
    "model_names = ['llava-onevision-qwen2-0.5b-ov-hf', 'InternVL2-1B', 'InternVL2_5-1B', 'Phi-3-Vision']\n",
    "\n",
    "# Initialize a dictionary to store the aggregated results\n",
    "aggregated_results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*openai_result.xlsx')\n",
    "    groups = {}\n",
    "    for i in file_paths:\n",
    "        group = get_group(os.path.basename(i))\n",
    "        groups.setdefault(group, []).append(os.path.basename(i))\n",
    "\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(groups.keys()) + 1):\n",
    "        all_combinations.extend(itertools.combinations(groups.keys(), r))\n",
    "\n",
    "    model_results = {}\n",
    "    for gp_element in all_combinations:\n",
    "        all_excel = {}\n",
    "        for indi_element in gp_element:\n",
    "            for i in groups[indi_element]:\n",
    "                df = pd.read_excel(f'{base_dir}/{model_name}/{i}')\n",
    "                print(df.hit)\n",
    "                hit = df.hit.tolist()\n",
    "                all_excel[os.path.basename(i).replace('.xlsx', '')] = hit\n",
    "        result = [int(any(values)) for values in zip(*all_excel.values())]\n",
    "        result = sum(result) / len(result)\n",
    "        model_results['-'.join(gp_element)] = result\n",
    "\n",
    "    # Add the model results to the aggregated results\n",
    "    for config, value in model_results.items():\n",
    "        if config not in aggregated_results:\n",
    "            aggregated_results[config] = {}\n",
    "        aggregated_results[config][model_name] = value\n",
    "\n",
    "# Convert the aggregated results into a DataFrame\n",
    "final_df = pd.DataFrame.from_dict(aggregated_results, orient='index').reset_index()\n",
    "final_df.rename(columns={'index': 'Configuration'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_path = f'{base_dir}/{dataset_name}_summary.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "def get_group(filename):\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    return match.group() if match else 'orig'\n",
    "\n",
    "def get_patch(filename):\n",
    "    pattern = r'row\\d+_col\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    return match.group() if match else 'orig'\n",
    "\n",
    "base_dir = './outputs'\n",
    "dataset_name = 'POPE_sample'\n",
    "model_names = ['llava-onevision-qwen2-0.5b-ov-hf', 'InternVL2-1B', 'InternVL2_5-1B', 'Phi-3-Vision']\n",
    "\n",
    "# Initialize a dictionary to store the aggregated results\n",
    "aggregated_results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*auxmatch.xlsx')\n",
    "    groups = {}\n",
    "    for i in file_paths:\n",
    "        group = get_group(os.path.basename(i))\n",
    "        groups.setdefault(group, []).append(os.path.basename(i))\n",
    "\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(groups.keys()) + 1):\n",
    "        all_combinations.extend(itertools.combinations(groups.keys(), r))\n",
    "\n",
    "    model_results = {}\n",
    "    for gp_element in all_combinations:\n",
    "        all_excel = {}\n",
    "        for indi_element in gp_element:\n",
    "            for i in groups[indi_element]:\n",
    "                df = pd.read_excel(f'{base_dir}/{model_name}/{i}')\n",
    "                df.score.astype(int)\n",
    "                hit = df.score.tolist()\n",
    "                all_excel[os.path.basename(i).replace('.xlsx', '')] = hit\n",
    "        result = [int(any(values)) for values in zip(*all_excel.values())]\n",
    "        result = sum(result) / len(result)\n",
    "        model_results['-'.join(gp_element)] = result\n",
    "\n",
    "    # Add the model results to the aggregated results\n",
    "    for config, value in model_results.items():\n",
    "        if config not in aggregated_results:\n",
    "            aggregated_results[config] = {}\n",
    "        aggregated_results[config][model_name] = value\n",
    "\n",
    "# Convert the aggregated results into a DataFrame\n",
    "final_df = pd.DataFrame.from_dict(aggregated_results, orient='index').reset_index()\n",
    "final_df.rename(columns={'index': 'Configuration'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_path = f'{base_dir}/{dataset_name}_summary.csv'\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: llava_hf_v1.5_7b\n",
      "Processing model: InternVL2-1B\n",
      "Processing model: Idefics3-8B-Llama3\n",
      "Processing model: Phi-3.5-Vision\n",
      "Summary file saved to ./outputs/summary_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Function to extract the group (e.g., 'grid_2x2') from the filename\n",
    "def get_group(filename):\n",
    "    \"\"\"\n",
    "    Extracts the group information from a filename.\n",
    "    If not found, defaults to 'orig'.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to parse.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted group name or 'orig'.\n",
    "    \"\"\"\n",
    "    pattern = r'grid_\\d+x\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return 'orig'\n",
    "\n",
    "# Function to group files by extracted group information\n",
    "def group_files_by_group(file_paths):\n",
    "    \"\"\"\n",
    "    Groups filenames by their group (e.g., 'grid_2x2').\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): List of file paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary where keys are group names and values are lists of filenames.\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    for file_path in file_paths:\n",
    "        group = get_group(os.path.basename(file_path))\n",
    "        if group not in groups:\n",
    "            groups[group] = []\n",
    "        groups[group].append(os.path.basename(file_path))\n",
    "    return groups\n",
    "\n",
    "# Function to calculate hit rates for all combinations of groups\n",
    "def calculate_hit_rates(groups, base_dir, model_name):\n",
    "    \"\"\"\n",
    "    Calculates hit rates for all combinations of groups.\n",
    "\n",
    "    Args:\n",
    "        groups (dict): Dictionary of grouped filenames.\n",
    "        base_dir (str): Base directory of files.\n",
    "        model_name (str): Name of the model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with configuration names as keys and hit rates as values.\n",
    "    \"\"\"\n",
    "    # Generate all combinations of group keys\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(groups.keys()) + 1):\n",
    "        for combination in itertools.combinations(groups.keys(), r):\n",
    "            all_combinations.append(combination)\n",
    "\n",
    "    final_dict = {}\n",
    "    for gp_element in all_combinations:\n",
    "        all_excel = {}\n",
    "        for group in gp_element:\n",
    "            for filename in groups[group]:\n",
    "                file_path = f'{base_dir}/{model_name}/{filename}'\n",
    "                df = pd.read_excel(file_path)\n",
    "                if 'hit' in df.columns:\n",
    "                    if filename not in all_excel:\n",
    "                        all_excel[filename] = []\n",
    "                    all_excel[filename].extend(df['hit'].tolist())\n",
    "\n",
    "        # Calculate the hit rate\n",
    "        combined_results = []\n",
    "        for values in zip(*all_excel.values()):\n",
    "            combined_results.append(int(any(values)))\n",
    "\n",
    "        if len(combined_results) > 0:\n",
    "            hit_rate = sum(combined_results) / len(combined_results)\n",
    "        else:\n",
    "            hit_rate = 0\n",
    "\n",
    "        final_dict['-'.join(gp_element)] = hit_rate\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "# Main function to process all models and save one summary file\n",
    "def process_models_and_save_summary(base_dir, dataset_name, model_names):\n",
    "    \"\"\"\n",
    "    Processes all models and saves a single summary CSV file.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory of files.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        model_names (list): List of model names to process.\n",
    "    \"\"\"\n",
    "    summary_data = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        # Get all relevant file paths\n",
    "        file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*openai_result.xlsx')\n",
    "\n",
    "        # Group files by extracted group information\n",
    "        groups = group_files_by_group(file_paths)\n",
    "\n",
    "        # Calculate hit rates\n",
    "        model_results = calculate_hit_rates(groups, base_dir, model_name)\n",
    "\n",
    "        # Add results to summary data\n",
    "        for config, value in model_results.items():\n",
    "            if config not in summary_data:\n",
    "                summary_data[config] = {}\n",
    "            summary_data[config][model_name] = value\n",
    "\n",
    "    # Create a DataFrame from summary data\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data, orient='index')\n",
    "    summary_df.reset_index(inplace=True)\n",
    "    summary_df.rename(columns={'index': 'Configuration'}, inplace=True)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    output_file = f'{base_dir}/summary_results.csv'\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    print(f\"Summary file saved to {output_file}\")\n",
    "\n",
    "# Set parameters\n",
    "base_dir = './outputs'\n",
    "dataset_name = 'RealWorldQA'\n",
    "model_names = ['llava_hf_v1.5_7b', 'InternVL2-1B', 'Idefics3-8B-Llama3', 'Phi-3.5-Vision']\n",
    "\n",
    "# Run the main process\n",
    "process_models_and_save_summary(base_dir, dataset_name, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred (403) when calling the HeadObject operation: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download file\n",
    "bucket_name = \"laion-us-east-1\"\n",
    "file_key = \"indices/vit-l-14/imagePQ64/infos.json\"\n",
    "local_file_name = \"infos.json\"\n",
    "\n",
    "try:\n",
    "    s3.download_file(bucket_name, file_key, local_file_name)\n",
    "    print(f\"File downloaded: {local_file_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def compute_scores_for_dataset(directory, dataset_keyword):\n",
    "    \"\"\"\n",
    "    Compute scores for files related to a specific dataset.\n",
    "    \n",
    "    :param directory: Directory containing the Excel files.\n",
    "    :param dataset_keyword: Keyword to identify relevant files (e.g., \"RealWorldQA\").\n",
    "    :return: DataFrame containing configurations and their scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file matches the dataset keyword and ends with '_openai_result.xlsx'\n",
    "        if dataset_keyword in filename and filename.endswith(\"_openai_result.xlsx\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                # Read the Excel file\n",
    "                df = pd.read_excel(filepath)\n",
    "                \n",
    "                # Calculate the score\n",
    "                hit = df['hit'].tolist()\n",
    "                score = sum(hit) / len(hit) if len(hit) > 0 else 0\n",
    "                \n",
    "                # Extract the configuration name (remove dataset prefix and suffix)\n",
    "                config_name = filename.replace(f\"Phi-3-Vision_{dataset_keyword}_\", \"\").replace(\"_openai_result.xlsx\", \"\")\n",
    "                \n",
    "                # Append the configuration and score to the results\n",
    "                results.append({\"Configuration\": config_name, \"Score\": score})\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Example usage\n",
    "directory_path = './outputs/Phi-3-Vision/'  # Replace with your actual directory path\n",
    "dataset_keyword = 'RealWorldQA'  # Keyword to filter relevant files\n",
    "\n",
    "results_df = compute_scores_for_dataset(directory_path, dataset_keyword)\n",
    "results_df.loc[results_df['Configuration'] == 'openai_result.xlsx', 'Configuration'] = 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def compute_scores_pivot(directory, dataset_keyword):\n",
    "    \"\"\"\n",
    "    Compute scores for all models and configurations and return a pivot table with 'original' at the top.\n",
    "\n",
    "    :param directory: Directory containing subdirectories for models.\n",
    "    :param dataset_keyword: Keyword to identify relevant files (e.g., \"RealWorldQA\").\n",
    "    :return: Pivot table DataFrame with configurations as rows and models as columns.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Loop through all subdirectories (models) in the directory\n",
    "    for model_name in os.listdir(directory):\n",
    "        model_path = os.path.join(directory, model_name)\n",
    "        \n",
    "        # Ensure it is a directory\n",
    "        if os.path.isdir(model_path):\n",
    "            # Process files within the model directory\n",
    "            for filename in os.listdir(model_path):\n",
    "                # Check if the file matches the dataset keyword and ends with '_openai_result.xlsx'\n",
    "                if dataset_keyword in filename and filename.endswith(\"_openai_result.xlsx\"):\n",
    "                    filepath = os.path.join(model_path, filename)\n",
    "                    try:\n",
    "                        # Read the Excel file\n",
    "                        df = pd.read_excel(filepath)\n",
    "\n",
    "                        # Calculate the score\n",
    "                        hit = df['hit'].tolist()\n",
    "                        score = sum(hit) / len(hit) if len(hit) > 0 else 0\n",
    "\n",
    "                        # Extract the configuration name (remove dataset prefix and suffix)\n",
    "                        config_name = filename.replace(f\"{model_name}_{dataset_keyword}_\", \"\").replace(\"_openai_result.xlsx\", \"\")\n",
    "\n",
    "                        # Append the model, configuration, and score to the results\n",
    "                        results.append({\"Model\": model_name, \"Configuration\": config_name, \"Score\": score})\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {filepath}: {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Handle specific configuration naming\n",
    "    results_df.loc[results_df['Configuration'] == 'openai_result.xlsx', 'Configuration'] = 'original'\n",
    "\n",
    "    # Create a pivot table with Configuration as rows and Models as columns\n",
    "    pivot_df = results_df.pivot(index='Configuration', columns='Model', values='Score').reset_index()\n",
    "\n",
    "    # Move 'original' configuration to the top\n",
    "    original_row = pivot_df[pivot_df['Configuration'] == 'original']\n",
    "    non_original_rows = pivot_df[pivot_df['Configuration'] != 'original']\n",
    "    pivot_df = pd.concat([original_row, non_original_rows], axis=0).reset_index(drop=True)\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "# Example usage\n",
    "directory_path = './outputs/'  # Replace with your actual directory path\n",
    "dataset_keyword = 'BLINK_sample'  # Keyword to filter relevant files\n",
    "\n",
    "# Compute scores and get the pivot table\n",
    "pivot_results_df = compute_scores_pivot(directory_path, dataset_keyword)\n",
    "\n",
    "# Display the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_results_df.to_csv(\"./BLINK_sample_per_grid_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_score(reference_captions, generated_captions):\n",
    "    reference_captions = {\"0\": ast.literal_eval(reference_captions)}\n",
    "    generated_captions = {\"0\": [generated_captions]}\n",
    "\n",
    "    scores, detailed_scores = bleu_scorer.compute_score(reference_captions, generated_captions)\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "def compute_scores_pivot(directory, dataset_keyword):\n",
    "    \"\"\"\n",
    "    Compute scores for all models and configurations and return a pivot table with 'original' at the top.\n",
    "\n",
    "    :param directory: Directory containing subdirectories for models.\n",
    "    :param dataset_keyword: Keyword to identify relevant files (e.g., \"RealWorldQA\").\n",
    "    :return: Pivot table DataFrame with configurations as rows and models as columns.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Loop through all subdirectories (models) in the directory\n",
    "    for model_name in os.listdir(directory):\n",
    "        model_path = os.path.join(directory, model_name)\n",
    "        \n",
    "        # Ensure it is a directory\n",
    "        if os.path.isdir(model_path):\n",
    "            # Process files within the model directory\n",
    "            for filename in os.listdir(model_path):\n",
    "                # Check if the file matches the dataset keyword and ends with '_openai_result.xlsx'\n",
    "                if dataset_keyword in filename and filename.endswith(\"_openai_result.xlsx\"):\n",
    "                    filepath = os.path.join(model_path, filename)\n",
    "                    try:\n",
    "                        # Read the Excel file\n",
    "                        df = pd.read_excel(filepath)\n",
    "\n",
    "                        # Calculate the score\n",
    "                        hit = df['hit'].tolist()\n",
    "                        score = sum(hit) / len(hit) if len(hit) > 0 else 0\n",
    "\n",
    "                        # Extract the configuration name (remove dataset prefix and suffix)\n",
    "                        config_name = filename.replace(f\"{model_name}_{dataset_keyword}_\", \"\").replace(\"_openai_result.xlsx\", \"\")\n",
    "\n",
    "                        # Append the model, configuration, and score to the results\n",
    "                        results.append({\"Model\": model_name, \"Configuration\": config_name, \"Score\": score})\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {filepath}: {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Handle specific configuration naming\n",
    "    results_df.loc[results_df['Configuration'] == 'openai_result.xlsx', 'Configuration'] = 'original'\n",
    "\n",
    "    # Create a pivot table with Configuration as rows and Models as columns\n",
    "    pivot_df = results_df.pivot(index='Configuration', columns='Model', values='Score').reset_index()\n",
    "\n",
    "    # Move 'original' configuration to the top\n",
    "    original_row = pivot_df[pivot_df['Configuration'] == 'original']\n",
    "    non_original_rows = pivot_df[pivot_df['Configuration'] != 'original']\n",
    "    pivot_df = pd.concat([original_row, non_original_rows], axis=0).reset_index(drop=True)\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "# Example usage\n",
    "directory_path = './outputs/'  # Replace with your actual directory path\n",
    "dataset_keyword = 'BLINK_sample'  # Keyword to filter relevant files\n",
    "\n",
    "# Compute scores and get the pivot table\n",
    "pivot_results_df = compute_scores_pivot(directory_path, dataset_keyword)\n",
    "\n",
    "# Display the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './outputs'\n",
    "model_name = 'InternVL2-1B' # llava_hf_v1.5_7b, InternVL2-1B, Idefics3-8B-Llama3, Phi-3.5-Vision\n",
    "dataset_name = 'BLINK_sample'\n",
    "file_paths = glob.glob(f'{base_dir}/{model_name}/{model_name}_{dataset_name}_*openai_result.xlsx')\n",
    "groups = {}\n",
    "for i in file_paths:\n",
    "    group = get_group(os.path.basename(i))\n",
    "    if group not in groups:\n",
    "        groups[group] = [os.path.basename(i)]\n",
    "    else:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
