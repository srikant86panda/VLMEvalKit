{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def md5(s):\n",
    "    hash = hashlib.new('md5')\n",
    "    if os.path.exists(s):\n",
    "        with open(s, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(2**20), b''):\n",
    "                hash.update(chunk)\n",
    "    else:\n",
    "        hash.update(s.encode('utf-8'))\n",
    "    return str(hash.hexdigest())\n",
    "\n",
    "def encode_image(img, target_size=-1, fmt='JPEG'):\n",
    "    # if target_size == -1, will not do resizing\n",
    "    # else, will set the max_size ot (target_size, target_size)\n",
    "    if img.mode in ('RGBA', 'P'):\n",
    "        img = img.convert('RGB')\n",
    "    if target_size > 0:\n",
    "        img.thumbnail((target_size, target_size))\n",
    "    img_buffer = io.BytesIO()\n",
    "    img.save(img_buffer, format=fmt)\n",
    "    image_data = img_buffer.getvalue()\n",
    "    ret = base64.b64encode(image_data).decode('utf-8')\n",
    "    return ret\n",
    "\n",
    "\n",
    "def encode_image_to_base64(img, target_size=-1):\n",
    "    return encode_image(img, target_size=target_size)\n",
    "\n",
    "\n",
    "def decode_base64(base64_string, target_size=-1):\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    if image.mode in ('RGBA', 'P'):\n",
    "        image = image.convert('RGB')\n",
    "    if target_size > 0:\n",
    "        image.thumbnail((target_size, target_size))\n",
    "    return image\n",
    "\n",
    "\n",
    "def decode_base64_to_image(base64_string, target_size=-1):\n",
    "    image = decode_base64(base64_string, target_size=target_size)\n",
    "    return image\n",
    "\n",
    "def split_image(img, target_section=None, grid_size=(3, 3), encode=False):\n",
    "    \"\"\"\n",
    "    Split an image into patches based on a given grid size.\n",
    "\n",
    "    Args:\n",
    "        img: PIL.Image object.\n",
    "        grid_size: Tuple specifying the number of rows and columns for the grid (rows, columns).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of cropped patches.\n",
    "    \"\"\"\n",
    "    width, height = img.size\n",
    "    num_rows, num_cols = grid_size\n",
    "\n",
    "    if num_rows < 1 or num_cols < 1:\n",
    "        raise ValueError(\"grid_size must contain positive integers for rows and columns.\")\n",
    "\n",
    "    # Calculate patch dimensions\n",
    "    patch_width = width / num_cols\n",
    "    patch_height = height / num_rows\n",
    "\n",
    "    grid_name = f'grid_{grid_size[0]}x{grid_size[1]}'\n",
    "    # Generate patches\n",
    "    patches = {}\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            left = int(col * patch_width)\n",
    "            upper = int(row * patch_height)\n",
    "            right = int((col + 1) * patch_width)\n",
    "            lower = int((row + 1) * patch_height)\n",
    "            section_name = f\"row{row+1}_col{col+1}\"\n",
    "            patches[f'{grid_name}_{section_name}'] = encode_image_to_base64(img.crop((left, upper, right, lower))) if encode else img.crop((left, upper, right, lower))\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file_path: /home/srikapan/LMUData/RealWorldQA.tsv written successfully, md5: 4de008f55dc4fd008ca9e15321dc44b7\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x1_row1_col1.tsv written successfully, md5: 23dd6bb085d7fc34b4501450285c2b65\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x1_row2_col1.tsv written successfully, md5: 1bb1973c44a95f26281afcbe5c1b0344\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_1x2_row1_col1.tsv written successfully, md5: 05f27eaaabbfda0b64510b5a40fd7904\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_1x2_row1_col2.tsv written successfully, md5: e64007802642df4f3bc8bf82f3470e09\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x2_row1_col1.tsv written successfully, md5: b9d02c2e82afbae3d8574fbebed3e1e6\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x2_row1_col2.tsv written successfully, md5: 7cdc6b709903e7be066dbc7862bb9a00\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x2_row2_col1.tsv written successfully, md5: 28d19d380f1e13e5266008374506c3ba\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_2x2_row2_col2.tsv written successfully, md5: 1fe65200b3cb57d521829472f6375164\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row1_col1.tsv written successfully, md5: 0ef5a1f5a2caefeb623177fa5cc3dd51\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row1_col2.tsv written successfully, md5: 469bde7d612953e032cf46394de307dd\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row1_col3.tsv written successfully, md5: 73dd3dbbd3ebf85369360902acbfafbf\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row2_col1.tsv written successfully, md5: 8116d7ab702a9bc1307af26d6b6aa5e2\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row2_col2.tsv written successfully, md5: 06a9ddb5d5b6769849cb7f913e724ad8\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row2_col3.tsv written successfully, md5: a851436bcb159683591fff6c758bdb78\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row3_col1.tsv written successfully, md5: 0c4e03b4c7a4b9375c0811b897bfa769\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row3_col2.tsv written successfully, md5: 4cf68f9ed12af00e55c917f7140f0b4b\n",
      "op_file_path: /home/srikapan/LMUData/RealWorldQA_grid_3x3_row3_col3.tsv written successfully, md5: 77483b061dd0dca5d08e8cddef7f15a3\n"
     ]
    }
   ],
   "source": [
    "## RealWorldQA pre-processing\n",
    "file_path = '/home/srikapan/LMUData/RealWorldQA.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "all_list_of_imgs = {}\n",
    "all_list_of_imgs['image'] = df.image.tolist()\n",
    "\n",
    "for img in df['image']:\n",
    "    all_crops = {}\n",
    "    img = decode_base64_to_image(img)\n",
    "    img_crops = split_image(img, grid_size=(2, 1), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(1, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(2, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(3, 3), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "\n",
    "    for label, crop in all_crops.items():\n",
    "        if label not in all_list_of_imgs:\n",
    "            all_list_of_imgs[label] = [crop]\n",
    "        else:\n",
    "            all_list_of_imgs[label].append(crop)\n",
    "\n",
    "print(f'original file_path: {file_path} written successfully, md5: {md5(file_path)}')\n",
    "for label, img_list in all_list_of_imgs.items():\n",
    "    if label != 'image':\n",
    "        temp_df = df.copy()\n",
    "        temp_df['image'] = img_list\n",
    "        op_file_path = f'{os.path.dirname(file_path)}/{os.path.basename(file_path).split(\".\")[0]}_{label}.tsv'\n",
    "        temp_df.to_csv(op_file_path, sep='\\t')\n",
    "        print(f'op_file_path: {op_file_path} written successfully, md5: {md5(op_file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file_path: /home/srikapan/LMUData/COCO_VAL_sample.tsv written successfully, md5: 5b6ed6e5f35024d003804372a13533c4\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x1_row1_col1.tsv written successfully, md5: 9999f71c872ada7193ac6f96e9c4030e\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x1_row2_col1.tsv written successfully, md5: 18ec1a575214bab28a2e50003e957299\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_1x2_row1_col1.tsv written successfully, md5: 6b02299cf429ca0ad398d01603688448\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_1x2_row1_col2.tsv written successfully, md5: 9ebee03bedc40d91c2fd08ff3dee1102\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x2_row1_col1.tsv written successfully, md5: f5e674127cc699ebfb77f66ba5581868\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x2_row1_col2.tsv written successfully, md5: 4c9f1408805b62e02695efe5cd546a1b\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x2_row2_col1.tsv written successfully, md5: b6a16dc21248550c2c287ffc5a6adeb3\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_2x2_row2_col2.tsv written successfully, md5: 149df47164d35499bfbf36481b2e8f5c\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row1_col1.tsv written successfully, md5: 7b5807ef64318afbe27858e48507eaae\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row1_col2.tsv written successfully, md5: e1640bc9276cf806e80248169b90b3d4\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row1_col3.tsv written successfully, md5: d12c40a377dcc8593ba1d745cd3867f7\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row2_col1.tsv written successfully, md5: 0359f4323e287b33c06fde01d2d8ca39\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row2_col2.tsv written successfully, md5: ffe43118269c9af4d703c0297a492053\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row2_col3.tsv written successfully, md5: 56ef9a830da851d0d225e6363d4ebe47\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row3_col1.tsv written successfully, md5: 3d81fb12514a28c410ba84f8b473087a\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row3_col2.tsv written successfully, md5: 830196882c5b8774a32a4f8ead8c6370\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_sample_grid_3x3_row3_col3.tsv written successfully, md5: f5f4996a9e79d8c90708f937f16d7936\n"
     ]
    }
   ],
   "source": [
    "## COCO sample pre-processing\n",
    "file_path = '/home/srikapan/LMUData/COCO_VAL_sample.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "all_list_of_imgs = {}\n",
    "all_list_of_imgs['image'] = df.image.tolist()\n",
    "\n",
    "for img in df['image']:\n",
    "    all_crops = {}\n",
    "    img = decode_base64_to_image(img)\n",
    "    img_crops = split_image(img, grid_size=(2, 1), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(1, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(2, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(3, 3), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "\n",
    "    for label, crop in all_crops.items():\n",
    "        if label not in all_list_of_imgs:\n",
    "            all_list_of_imgs[label] = [crop]\n",
    "        else:\n",
    "            all_list_of_imgs[label].append(crop)\n",
    "\n",
    "print(f'original file_path: {file_path} written successfully, md5: {md5(file_path)}')\n",
    "for label, img_list in all_list_of_imgs.items():\n",
    "    if label != 'image':\n",
    "        temp_df = df.copy()\n",
    "        temp_df['image'] = img_list\n",
    "        op_file_path = f'{os.path.dirname(file_path)}/{os.path.basename(file_path).split(\".\")[0]}_{label}.tsv'\n",
    "        temp_df.to_csv(op_file_path, sep='\\t')\n",
    "        print(f'op_file_path: {op_file_path} written successfully, md5: {md5(op_file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file_path: /home/srikapan/LMUData/COCO_VAL.tsv written successfully, md5: 72a5079dead060269ac222c5aa5128af\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x1_row1_col1.tsv written successfully, md5: a661f336357cc033c0ac362ce43c8934\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x1_row2_col1.tsv written successfully, md5: a81faa8e1616fc8893a1c9208358a8fb\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_1x2_row1_col1.tsv written successfully, md5: 220648e71ea09065a5e6c984c98d257d\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_1x2_row1_col2.tsv written successfully, md5: c0ccc20c7af4b1382a6c128ad449dd57\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x2_row1_col1.tsv written successfully, md5: a37fc00dab224f9ae041172b25ac8735\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x2_row1_col2.tsv written successfully, md5: f43fb8f80df717a53d1f441047cfd4c8\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x2_row2_col1.tsv written successfully, md5: 407cc5d794fb986fac4bebfd2eef09ca\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_2x2_row2_col2.tsv written successfully, md5: 0924bda06630c614fd74e22d4dd43690\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row1_col1.tsv written successfully, md5: 54aab03cd5a0efe08032f89a88131e67\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row1_col2.tsv written successfully, md5: 80d808b6c9246ecd013503c3ed553a9a\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row1_col3.tsv written successfully, md5: 039933f1adb271f7af61481933ba9e79\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row2_col1.tsv written successfully, md5: 27aadc2b62fd451690b61c6217d36a41\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row2_col2.tsv written successfully, md5: f8e7abca5ed106f5303e1a8c6a0c720e\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row2_col3.tsv written successfully, md5: 92283d89cd17aab740d0578adca10fad\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row3_col1.tsv written successfully, md5: d50f7854d5d0b27ded7c9ab46c838791\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row3_col2.tsv written successfully, md5: d43351ea6958510794c13db54f926b7c\n",
      "op_file_path: /home/srikapan/LMUData/COCO_VAL_grid_3x3_row3_col3.tsv written successfully, md5: cd8f8c222ca98a8375454f23d89cfb08\n"
     ]
    }
   ],
   "source": [
    "## COCO pre-processing\n",
    "file_path = '/home/srikapan/LMUData/COCO_VAL.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "all_list_of_imgs = {}\n",
    "all_list_of_imgs['image'] = df.image.tolist()\n",
    "\n",
    "for img in df['image']:\n",
    "    all_crops = {}\n",
    "    img = decode_base64_to_image(img)\n",
    "    img_crops = split_image(img, grid_size=(2, 1), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(1, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(2, 2), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "    img_crops = split_image(img, grid_size=(3, 3), encode=True)\n",
    "    all_crops.update(img_crops)\n",
    "\n",
    "    for label, crop in all_crops.items():\n",
    "        if label not in all_list_of_imgs:\n",
    "            all_list_of_imgs[label] = [crop]\n",
    "        else:\n",
    "            all_list_of_imgs[label].append(crop)\n",
    "\n",
    "print(f'original file_path: {file_path} written successfully, md5: {md5(file_path)}')\n",
    "for label, img_list in all_list_of_imgs.items():\n",
    "    if label != 'image':\n",
    "        temp_df = df.copy()\n",
    "        temp_df['image'] = img_list\n",
    "        op_file_path = f'{os.path.dirname(file_path)}/{os.path.basename(file_path).split(\".\")[0]}_{label}.tsv'\n",
    "        temp_df.to_csv(op_file_path, sep='\\t')\n",
    "        print(f'op_file_path: {op_file_path} written successfully, md5: {md5(op_file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_VAL_grid_2x1_row1_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x1_row1_col1.tsv\n",
      "COCO_VAL_grid_2x1_row2_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x1_row2_col1.tsv\n",
      "COCO_VAL_grid_1x2_row1_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_1x2_row1_col1.tsv\n",
      "COCO_VAL_grid_1x2_row1_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_1x2_row1_col2.tsv\n",
      "COCO_VAL_grid_2x2_row1_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x2_row1_col1.tsv\n",
      "COCO_VAL_grid_2x2_row1_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x2_row1_col2.tsv\n",
      "COCO_VAL_grid_2x2_row2_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x2_row2_col1.tsv\n",
      "COCO_VAL_grid_2x2_row2_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_2x2_row2_col2.tsv\n",
      "COCO_VAL_grid_3x3_row1_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row1_col1.tsv\n",
      "COCO_VAL_grid_3x3_row1_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row1_col2.tsv\n",
      "COCO_VAL_grid_3x3_row1_col3 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row1_col3.tsv\n",
      "COCO_VAL_grid_3x3_row2_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row2_col1.tsv\n",
      "COCO_VAL_grid_3x3_row2_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row2_col2.tsv\n",
      "COCO_VAL_grid_3x3_row2_col3 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row2_col3.tsv\n",
      "COCO_VAL_grid_3x3_row3_col1 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row3_col1.tsv\n",
      "COCO_VAL_grid_3x3_row3_col2 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row3_col2.tsv\n",
      "COCO_VAL_grid_3x3_row3_col3 : https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/COCO_VAL_grid_3x3_row3_col3.tsv\n"
     ]
    }
   ],
   "source": [
    "dic = {'COCO_VAL_grid_2x1_row1_col1' : 'a661f336357cc033c0ac362ce43c8934',\n",
    "'COCO_VAL_grid_2x1_row2_col1' : 'a81faa8e1616fc8893a1c9208358a8fb',\n",
    "'COCO_VAL_grid_1x2_row1_col1' : '220648e71ea09065a5e6c984c98d257d',\n",
    "'COCO_VAL_grid_1x2_row1_col2' : 'c0ccc20c7af4b1382a6c128ad449dd57',\n",
    "'COCO_VAL_grid_2x2_row1_col1' : 'a37fc00dab224f9ae041172b25ac8735',\n",
    "'COCO_VAL_grid_2x2_row1_col2' : 'f43fb8f80df717a53d1f441047cfd4c8',\n",
    "'COCO_VAL_grid_2x2_row2_col1' : '407cc5d794fb986fac4bebfd2eef09ca',\n",
    "'COCO_VAL_grid_2x2_row2_col2' : '0924bda06630c614fd74e22d4dd43690',\n",
    "'COCO_VAL_grid_3x3_row1_col1' : '54aab03cd5a0efe08032f89a88131e67',\n",
    "'COCO_VAL_grid_3x3_row1_col2' : '80d808b6c9246ecd013503c3ed553a9a',\n",
    "'COCO_VAL_grid_3x3_row1_col3' : '039933f1adb271f7af61481933ba9e79',\n",
    "'COCO_VAL_grid_3x3_row2_col1' : '27aadc2b62fd451690b61c6217d36a41',\n",
    "'COCO_VAL_grid_3x3_row2_col2' : 'f8e7abca5ed106f5303e1a8c6a0c720e',\n",
    "'COCO_VAL_grid_3x3_row2_col3' : '92283d89cd17aab740d0578adca10fad',\n",
    "'COCO_VAL_grid_3x3_row3_col1' : 'd50f7854d5d0b27ded7c9ab46c838791',\n",
    "'COCO_VAL_grid_3x3_row3_col2' : 'd43351ea6958510794c13db54f926b7c',\n",
    "'COCO_VAL_grid_3x3_row3_col3' : 'cd8f8c222ca98a8375454f23d89cfb08',}\n",
    "\n",
    "for i in dic.keys():\n",
    "    print(i, ':', f'https://huggingface.co/datasets/Srikant86/VLMEval/resolve/main/COCO_VAL/{i}.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLMEvalKit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
